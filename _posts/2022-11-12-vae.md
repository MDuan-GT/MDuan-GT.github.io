---
title: 'Variational autoencoders'
date: 2022-11-12
permalink: /posts/vae/
tags:
  - tutorial
  - deep learning
  - machine learning
  - probabilistic models
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders (VAEs), introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114_) are a class of probabilistic models that find latent, low-dimensional representations of data. VAEs are thus a method for performing [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce data down to their [intrinsice dimensionality](https://mbernste.github.io/posts/intrinsic_dimensionality/). 

There are two complimentary ways of viewing variational autoencoders:
1. **Probabilistic generative model:** VAEs can be viewed as a probabilistic generative model of independent, identically distributed samples, $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m$. Each sample, $\boldsymbol{x}_i$, is associated with a latent (i.e. unobserved), lower-dimensional representation $\boldsymbol{z}_i$. Notably, each observed sample $\boldsymbol{x}_i$ is of higher dimension than its associated lower dimensional representation $\boldsymbol{z}_i$.  Variational autoencoders define a joint distribution $p(\boldsymbol{x}, \boldsymbol{z})$ and enables efficient computation of:
  * An approximation to the posterior $p(\boldsymbol{z} \mid \boldsymbol{x})$
  * The conditional distribution $p(\boldsymbol{x} \mid \boldsymbol{z})$
2. **Autoencoder:** As their name suggests, a VAE can be viewed as an autoencoder. Unlike standard autoencoders, which are deterministic, VAEs are probabilistic; Given an input sample, $\boldsymbol{x}_i$, its encoded, latent representation $\boldsymbol{z}_i$ is randomly generated.

In this blog post we will present VAEs through both of these lenses. We will present an example of running VAEs on MNIST. Finally, we will discuss applications of VAEs in computational biology. Specfically, we will discuss the [scVI method](https://docs.scvi-tools.org/en/stable/user_guide/models/scvi.html) by [Lopez et al. (2018)](https://www.nature.com/articles/s41592-018-0229-2), which is a VAE-based tool used to analyze [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single_cell_sequencing) data.

VAEs as probabilistic generative models
---------------------------------------

At their core, VAEs describe families of probability distributions over real-valued vectors in $\mathbb{R}^J$.  The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the parameters, $\boldsymbol{\psi}$, of another distribution used to sample $\boldsymbol{x}$. Crucially, we construct $\psi$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\psi} &:= f_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim \mathcal{D}(\boldsymbol{\psi}) \end{align*}$$

where $\mathcal{D}$ is a parametric distribution and $f$ is a neural network parameterized by a set of parameters $\theta$. As one example, $\mathcal{D}$ may be a Gaussian distribution with unit variance and $\psi$ is the mean of this distribution.  Here's a schematic illustration of the generative process:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process_shapes.png" alt="drawing" width="700"/></center>

&nbsp;

This generative process can be visualized graphically below:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>

&nbsp;

Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ may be simple (e.g., both simply normal distributions), the marginal distribution of $\boldsymbol{x}$ becomes complex:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>

&nbsp;

This complexity is a result of the non-linear mapping between $\boldsymbol{z}$ and $\psi$ implemented via the neural network!

Inference
---------

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ that we assume was generated by a VAE. We may be interested in two central tasks:
1. For fixed $\theta$, for each $\boldsymbol{x}\_i$, compute the posterior distribution $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$
2. Find the maximum likelihood estimates of $\theta$

Unfortunately, for a fixed $\theta$, solving for the posterior $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$ using Bayes Theorem is intractible due the denominator in the formula for Bayes Theorem requires marginalizing over $\boldsymbol{z}_i$:

$$p_\theta(\boldsymbol{z}_i \mid \boldsymbol{x}_i) = \frac{p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i)p(\boldsymbol{z}_i)}{\int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i }$$

Similarly, it is difficult to estimate the values for $\theta$ by maximizing the data likelihood due to the need to marginalize over each $\boldsymbol{z}\_i$:

$$\begin{align*}\hat{\theta} &:= \text{argmax}_\theta \prod_{i=1}^m p_\theta(\boldsymbol{x}_i) \\ &= \text{argmax}_\theta \prod_{i=1}^m \int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i  \end{align*}$$

Variatonal autoencoders find approximate solutions to these intractible inference problems using [variational inference](https://mbernste.github.io/posts/variational_inference/). 

First, let's assume that $\theta$ is fixed and attempt to approximate $p\_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Variational inference is a method for performing such approximations by first choosing a set of probability distributions, $\mathcal{Q}$, called the _variational family_, and then finding the distribution $q(\boldsymbol{z}) \in \mathcal{Q}$ that is "closest to" $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$. Variational inference uses the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $q(\boldsymbol{z})$ and $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$ as its measure of "closeness". Thus, the goal of variational inference is to minimize the KL-divergence. It turns out that the task of minimizing the KL-divergence is equivalent to the task of maximizing a quantity called the [evidence lower bound (ELBO)](https://mbernste.github.io/posts/elbo/), which is defined as

$$\begin{align*} \text{ELBO}(q) &:= E_{\boldsymbol{z} \sim q}\left[ \sum_{i=1}^m \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \sum_{i=1}^m \log q(\boldsymbol{z}_i) \right] \\ &= \sum_{i=1}^m E_{\boldsymbol{z} \sim q} \left[\log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q(\boldsymbol{z}_i) \right] \end{align*}$$

Thus, variational inference entails finding

$$\hat{q} := \text{arg max}_{q \in \mathcal{Q}} \ \text{ELBO}(q)$$

Now, so far we have assumed that $\theta$ is fixed. Is it possible to find both $q$ and $\theta$ jointly? As we discuss in a [previous post on variational inference](https://mbernste.github.io/posts/reparameterization_vi/), it is valid to define the ELBO as a function of _both_ $q$ and $\theta$ and then maximize the ELBO with respect to both of these parameters:

$$\hat{q}, \hat{\theta} := \text{arg max}_{q, \theta} \ \text{ELBO}(q, \theta)$$

The reason for this is that the ELBO is a _lower bound_ on the marginal log-likelihood $p_\theta(x_1, \dots, x_n}$ and thus, optimizing the ELBO with respect to $\theta$ increases the lower bound of the log-likelihood. 


### Variational family used by VAEs

VAEs use a variational family with the following form:

$$\mathcal{Q} := \{N(g_\phi(\boldsymbol{x}), \exp(h_\phi(\boldsymbol{x})) \boldsymbol{I}) \mid \phi \in \mathbb{R}^R\}$$

where $g_{\phi}$ and $h_{\phi}$ are neural networks that encode the mean, $\boldsymbol{\mu}$, and the logarithm of the variance, $\log \boldsymbol{\sigma}^2$, of the approximate posterior. Here, $R$ is the number of parameters to these neural networks. Said a second way, we define $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ as

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(g_\phi(\boldsymbol{x}), \exp(h_\phi(\boldsymbol{x})) \boldsymbol{I})$$

Said a third way, the approximate posterior distribution can be sampled via the following process:

$$\begin{align*}\boldsymbol{\mu} &:=  g_\phi(\boldsymbol{x}) \\ \log \boldsymbol{\sigma}^2 &:= h_\phi(\boldsymbol{x}) \\ \boldsymbol{z} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma^2}\boldsymbol{I}) \end{align*}$$

This can be visualized as follows:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encoder.png" alt="drawing" width="700"/></center>

&nbsp;

Thus, maximizing the ELBO over $\mathcal{Q}$ reduces to maximizing over the neural network parameters $\phi$ (in addition to $\theta$ as discussed previously):

$$\begin{align*}\hat{\phi}, \hat{\theta} &= \text{arg max}_{\phi, \theta} \  \text{ELBO}(\phi, \theta) \\ &:= \text{arg max}_{\phi, \theta} \  \sum_{i=1}^m E_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \end{align*}$$

### Maximizing the ELBO

Now that we've set up the optimization problem, we need to solve it. Unfortunately, the expecation present in the ELBO makes this difficult as it requires integrating over all possible values for $\boldsymbol{z}_i$:

$$\begin{align*}\text{ELBO}(\phi, \theta) &= \sum_{i=1}^m E_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \sum_{i=1}^m \int_{\boldsymbol{z}_i}   q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \end{align*}$$

We address this challenge by using the **reparameterization gradient** method. We will briefly review this method; however, see my [previous blog post](https://mbernste.github.io/posts/reparameterization_vi/) for a detailed explanation. In brief, the reparameterization method maximizes the ELBO via stochastic gradient ascent in which stochastic gradients are formulated by first performing the **reparameterization trick** followed by Monte Carlo sampling.  The reparameterization trick works as follows: we "reparameterize" the distribution $q_\phi(z)$ in terms of a surrogate random variable $\epsilon \sim \mathcal{D}$ and a determinstic function $g$ in such a way that sampling $z$ from $q_\phi(z)$ is performed as follows:

$$\begin{align*}\epsilon &\sim \mathcal{D} \\ z &:= g_\phi(\epsilon)\end{align*}$$

One way to think about this is that instead of sampling $z$ directly from our variational posterior $q_\phi(z)$, we "re-design" the generative process of $z$ such that we first sample a surrogate random variable $\epsilon$ and then transform $\epsilon$ into $z$ all while ensuring that in the end, the distribution of $z$ still follows $q_\phi$. Crucially, $\mathcal{D}$ must be something we can easily sample from such as a standard normal distribution. Following the reparameterization trick, we can re-write the ELBO as follows:

$$\text{ELBO}(\phi) := E_{\epsilon \sim \mathcal{D}} \left[ \log p(x, g_\phi(\epsilon)) - \log q_\phi(g_\phi(\epsilon)) \right]$$

We then approximate the ELBO via Monte Carlo sampling. That is, we can first sample random variables from our surrogate distribution $\mathcal{D}$:

$$\epsilon'_1, \dots, \epsilon'_L \sim \mathcal{D}$$

Then we can compute a Monte Carlo approximation to the ELBO:

$$\tilde{ELBO}(\phi) := \frac{1}{L} \sum_{l=1}^L \left[  \log p(x, g_\phi(\epsilon'_l)) - \log q_\phi(g_\phi(\epsilon'_l)) \right]$$ 

So long as $g_\phi$ is continuous with respect to $\phi$ and $p$ is continuous with respect to $z$, we can take gradient of this approximation:

$$\nabla_\phi \tilde{ELBO}(\phi) := \nabla_\phi \frac{1}{L} \sum_{l=1}^L \left[  \log p(x, g_\phi(\epsilon'_l)) - \log q_\phi(g_\phi(\epsilon'_l)) \right]$$

This gradient can then be used to perform gradient ascent. In VAE's we also have model parameters $\theta$ and this stochastic gradient becomes:

$$\nabla_{\phi, \theta} \tilde{ELBO}(\phi, \theta) := \nabla_{\phi, \theta} \frac{1}{L} \sum_{l=1}^L \left[  \log p_\theta(x, g_\phi(\epsilon'_l)) - \log q_\phi(g_\phi(\epsilon'_l)) \right]$$

To compute this gradient, we can apply [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) algorithms in combination with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)-based optimization, thus enabling us to utilize the full deep learning optimization toolkit! 

For the VAE model there are a few additional modifications we can make to reduce variance of the Monte Carlo gradients. Recall the VAEs we have considered in this blog post have defined $p(\boldsymbol{z})$ to be the standard normal distribution $N(\boldsymbol{0}, \boldsymbol{I})$. In this particular case, it turns out that the KL-divergence term above can be expressed analytically (See Theorem 1 in the Appendix to this post):

$$KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \mid\mid p(\boldsymbol{z})) = -\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right)$$

Note above the KL-divergence is calculated by summing over each dimension in the latent space. The full ELBO is:

$$\begin{align*} \text{ELBO}(\phi, \theta) &= \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right) + E_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right]\right] \end{align*}$$

Then,

$$\begin{align*}\text{ELBO}(\phi, \theta) &\approx \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right) + \frac{1}{L} \sum_{l=1}^L \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}'_{i,l}) \right] \right] \\ &= \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h_\phi(\boldsymbol{x}_i)_j - g_\phi(\boldsymbol{x}_i)_j^2 - h_\phi(\boldsymbol{x}_i)_j \right) + \frac{1}{L} \sum_{l=1}^L \left[\log p_\theta(\boldsymbol{x}_i \mid g_\phi(\boldsymbol{x}_{i}) + h_\phi(\boldsymbol{x}_i)\epsilon_{i,l}) \right] \right]\end{align*}$$


VAEs as autoencoders
---------------------

Example: Training a VAE on MNIST
--------------------------------

I implemented a VAE in [PyTorch](https://pytorch.org) and trained it on the MNIST dataset. These data consist of 28x28 pixel images of hand written digits. My VAE used a latent representation of length 100 (that is, $\boldsymbol{z} \in \mathbb{R}^{100}$.

After enough training, the algorithm was able to reconstruct images that were not included in the training data. Here's an example:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_MNIST_reconstruction.png" alt="drawing" width="600"/></center>

&nbsp;

As you can see, it did pretty well! The reconstructed image looks very much like the original image. This is notable because the input image was not in the training set. Thus, the model learned how to generalize the task of encoding and decoding samples.

Let's see what happens when we attempt to reconstruct images from their latent representation. In order to do so, we first sample $\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$ and then display $f_\theta(\boldsymbol(z))$:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/example_MNIST_generative_process.png" alt="drawing" width="900"/></center>

&nbsp;

As you can see, some of these digits resemble numbers. Notably, nobody has actually written these digits. They were drawn by the model!

Lastly, let's explore the latent space learned by the model. First, let's take the images of a "9" and "0", and encode them into the latent space by sampling from $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$. Let's let $\boldsymbol{z}_1$ and $\boldsymbol{z}_2 be the latent vectors for "9" and "0" respectively: 

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encode_9_0.png" alt="drawing" width="800"/></center>

&nbsp;

Then, let's interpolate between $\boldsymbol{z}\_1$ and $\boldsymbol{z}\_2$ and for each interpolated vector $\boldsymbol{z}'$ we'll compute $f_\theta(\boldsymbol{z})$:  

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_interpolate_9_to_0.png" alt="drawing" width="800"/></center>

&nbsp;


Interestingly, we see a smooth transition between the 9 to the 0! The latent representations that exist between 9 and 0 appear as sort of combined forms of these digits. 

