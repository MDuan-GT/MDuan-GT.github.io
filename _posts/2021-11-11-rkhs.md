---
title: 'Reproducing kernel Hilbert spaces'
date: 2021-11-09
permalink: /posts/rkhs/
tags:
  - tutorial
  - mathematics
  - functional analysis
---

THIS POST IS CURRENTLY UNDER CONSTRUCTION

Introduction
------------

If you're a practitioner of machine learning, then there is little doubt you have seen or used an algorithm that falls into the general category of [kernel methods](https://en.wikipedia.org/wiki/Kernel_method). The premier example of such a method being the [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine). When introduced to these algorithms, one is taught that one must provide the algorithm with a **kernel function** that, intuitively, computes a degree of "similarity" between the objects your are classifying (e.g., images or text documents).  

A slightly deeper introduction will explain that a kernel is actually projecting the objects into some, possibly infinite dimensional [vector space](https://mbernste.github.io/posts/vector_spaces/) and then performing an inner product on those vectors. That is, if $$\mathcal{X}$$ is the set of objects we are classifying, then a kernel, $K$, is a [positive-definite function](https://en.wikipedia.org/wiki/Positive-definite_function):

$$K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$$

for which 

$$K(x_1, x_2) = \langle \phi(x_1), \phi(x_2)\rangle$$

where $\phi(x)$ is a vector associated with object $x$ in some [vector space](https://mbernste.github.io/posts/vector_spaces/). 

To use a kernel method in practice, one can get pretty far with only this understanding; however, it is incomplete and to me, unsatisfying. What space are these objects projected into? How is a kernel function derived? 

To answer these questions, one must understand a mathematical object called a **reproducing kernel Hilbert space** (RKHS). These objects were a bit challenging for me to intuit, so in this post, I will explain the definition of a RKHS and the kernels that they produce. I will then discuss some of their properties in order to provide a foundation for better intuiting kernel methods beyond the introductory explanations often provided to new students.  

The reproducing kernel Hilbert space
------------------------------------

Let's breakdown the name, "reproducing kernel Hilbert space". First, we note that a reproducing kernel Hilbert space is, first and foremost, a Hilbert space. Recall from our previous blog post, that a Hilbert space is a vector space that is 1) equipped with an inner product (which induces a [norm]() on each vector) and 2) is a [complete space]().  More intuitively, a Hilbert space generalizes Euclidean spaces in that vectors are infinitely dense, one can compute the magnitude (i.e., norm) of each vector, and one can compare vectors using an inner product.

With this out of the way, we note that a reproducing kernel Hilbert space is a Hilbert space that consists of _functions_. Recall that a [vector space]() is any space of objects that can be added together and scaled according to a specific set of axioms and this can hold for sets of functions. 

Now a RKHS is not just _any_ Hilbert space of functions, rather it has to have a certain property as seen below in the definition for a RKHS:

<span style="color:#0060C6">**Definition 1 (Reproducing kernel Hilbert space):** We're given a Hilbert space $(\mathcal{H}, \mathcal{F}, \langle ., . \rangle)$ where $\mathcal{H}$ is the set of vectors, $$\mathcal{F}$$ are a set of scalars, $\langle ., . \rangle$ is an inner product on $\mathcal{H$}$, and $\forall f \in \mathcal{H}$, $f$ is a function $f : \mathbb{S} \rightarrow \mathbb{R}$, where $\mathcal{X}$ is some set. This Hilbert space is a **reproducing kernel Hilbert space** if given any $x \in \mathcal{X}$, the evaluation functional for $x$, $\delta_x(f) := f(x)$ (where $f \ in \mathcal{H})$, is continuous.</span>

Okay let's break this down.

Appendix: Proofs of properties of the RKHS and kernels
------------------------------------------------------


<span style="color:#0060C6">**Theorem 1 (Convergence of functions implies pointwise convergence):** Given a RKHS, $\mathcal{H}$, the following holds: $ \ \lim_{n \rightarrow \infty} \vert\vert f_n - f \vert\vert_{\mathcal{H}} = 0 \implies \forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$.</span>

We assume that we have a convergent sequence of functions that converges on some function $f$. That is, $\lim_{n \rightarrow \infty} \|f_n - f\|_{\mathcal{H}} = 0$. By the definition of the limit of a sequence, this means that

$$\forall \epsilon_2 > 0, \ \exists N \ \text{such that} \ n > N \implies  \vert\vert f_n - f  \vert\vert_{\mathcal{H}} < \epsilon_2$$

Let's keep this in mind as we look at the axioms of a RKHS. Specifically, we note that the axiom of RKHS means that 

$$\forall x \in \mathbb{R}, \ \forall \epsilon_1 > 0, \ \exists \Delta > 0 \ \text{such that} \ \forall f, \ 0 \lt \vert\vert f-g \vert\vert_{\mathcal{H}} \lt \Delta \implies \vert\delta_x(f) - \delta_x(g)\vert < \epsilon_1$$

Let us fix $\epsilon_1$ above to an arbitrary value and let $\epsilon_2 := \Delta$. Furthermore, let $x$ be a fixed arbitrary value. Then, because we have a convergence sequence of functions, $f_n$, we know that $\exists N \ \text{such that} \ n > N \implies \vert\vert f_n - f \vert\vert_{\mathcal{H}} < \Delta$. 

Based on the axiom of the RKHS, this also implies that 

$$\vert \delta_x(f_n) - \delta_x(f)\vert < \epsilon_1$$

and thus,

$$\vert f_n(x) - f(x)\vert < \epsilon_1$$

Now, since our choices of $\epsilon_1$ and $x$ were arbitrary, we see that 

$$\forall x, \ \forall \epsilon_1 > 0, \ \exists N \ \text{such that} \ n > N \implies \vert f_n(x) - f(x) \vert < \epsilon_1$$

This is the very definition of the limit, 

$$\forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$$

$\square$
