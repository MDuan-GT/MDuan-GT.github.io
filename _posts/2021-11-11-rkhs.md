---
title: 'Reproducing kernel Hilbert spaces'
date: 2021-11-09
permalink: /posts/rkhs/
tags:
  - tutorial
  - mathematics
  - functional analysis
---

THIS POST IS CURRENTLY UNDER CONSTRUCTION

Introduction
------------

If you're a practitioner of machine learning, then there is little doubt you have seen or used an algorithm that falls into the general category of [kernel methods](https://en.wikipedia.org/wiki/Kernel_method). The premier example of such a method being the [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine). When introduced to these algorithms, one is taught that one must provide the algorithm with a **kernel function** that, intuitively, computes a degree of "similarity" between the objects your are classifying (e.g., images or text documents).  

At a slightly deeper level, one is usually taught that the kernel function is actually projecting the objects that you are classifying into some, possibly infinite dimensional [vector space](https://mbernste.github.io/posts/vector_spaces/) and then performs an inner product on those vectors. That is, if $$\mathcal{X}$$ is the set of objects we are classifying, then a kernel, $K$, is a [positive-definition function](https://en.wikipedia.org/wiki/Positive-definite_function):

$$K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$$

for which 

$$K(x_1, x_2) = \langle \phi(x_1), \phi(x_2)\rangle$$

where $\phi(x)$ is a vector associated with object $x$ some [vector space](https://mbernste.github.io/posts/vector_spaces/). 

To use a kernel method in practice, one can get pretty far with only this understanding; however, if it is incomplete and to me, unsatisfying. What space are these objects projected into? How is a kernel function derived? 

To answer these questions, one must understand a mathematical object called a **reproducing kernel Hilbert space** (RKHS). These objects were a bit challenging for me to intuit, so in this post, I will explain the definition of a RKHS and the kernels that they produce. I will then discuss some of their properties in order to provide a foundation for better intuiting kernel methods beyond the introductory explanations often provided to new students.  

The reproducing kernel Hilbert space
------------------------------------


That is, if $$\mathcal{X}$$ is the set of objects we are classifying, then a kernel, $K$, is a [positive-definition function](https://en.wikipedia.org/wiki/Positive-definite_function):

$$K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$$

That is, for a given pair of items $x_1, x_2 \in \mathcal{X}$, the $K(x_1, x_2)$ is some measure of similarity between $x_1$ and $x_2$. 

A slightly more involved introduction to kernel methods will explain that the kernel function is actually performing an inner product on some pair of vectors associated with each objct in an alternative, possibly infinite-dimensional vector space. That is,

$$K(x_1, x_2) = \langle \phi(x_1), \phi(x_2)\rangle$$

where $\phi$ is a function that maps each object $x \in \mathcal{X}$ to some vector space.

Though this explanation is helpful to some extent, it is by no means complete; and to me, it isn't very satisfying. 




<span style="color:#0060C6">**Theorem 1 (Convergence of functions implies pointwise convergence):** Given a RKHS, $\mathcal{H}$, the following holds: $ \ \lim_{n \rightarrow \infty} \vert\vert f_n - f \vert\vert_{\mathcal{H}} = 0 \implies \forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$.</span>

We assume that we have a convergent sequence of functions that converges on some function $f$. That is, $\lim_{n \rightarrow \infty} \|f_n - f\|_{\mathcal{H}} = 0$. By the definition of the limit of a sequence, this means that

$$\forall \epsilon_2 > 0, \ \exists N \ \text{such that} \ n > N \implies  \vert\vert f_n - f  \vert\vert_{\mathcal{H}} < \epsilon_2$$

Let's keep this in mind as we look at the axioms of a RKHS. Specifically, we note that the axiom of RKHS means that 

$$\forall x \in \mathbb{R}, \ \forall \epsilon_1 > 0, \ \exists \Delta > 0 \ \text{such that} \ \forall f, \ 0 \lt \vert\vert f-g \vert\vert_{\mathcal{H}} \lt \Delta \implies \vert\delta_x(f) - \delta_x(g)\vert < \epsilon_1$$

Let us fix $\epsilon_1$ above to an arbitrary value and let $\epsilon_2 := \Delta$. Furthermore, let $x$ be a fixed arbitrary value. Then, because we have a convergence sequence of functions, $f_n$, we know that $\exists N \ \text{such that} \ n > N \implies \vert\vert f_n - f \vert\vert_{\mathcal{H}} < \Delta$. 

Based on the axiom of the RKHS, this also implies that 

$$\vert \delta_x(f_n) - \delta_x(f)\vert < \epsilon_1$$

and thus,

$$\vert f_n(x) - f(x)\vert < \epsilon_1$$

Now, since our choices of $\epsilon_1$ and $x$ were arbitrary, we see that 

$$\forall x, \ \forall \epsilon_1 > 0, \ \exists N \ \text{such that} \ n > N \implies \vert f_n(x) - f(x) \vert < \epsilon_1$$

This is the very definition of the limit, 

$$\forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$$

$\square$
