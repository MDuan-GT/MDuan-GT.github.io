---
title: 'Reproducing kernel Hilbert spaces'
date: 2021-11-09
permalink: /posts/rkhs/
tags:
  - tutorial
  - mathematics
  - functional analysis
---

THIS POST IS CURRENTLY UNDER CONSTRUCTION

Introduction
------------

If you're a practitioner of machine learning, then there is little doubt you have heard of, and likely used an algorithm that falls into the general category of **kernel methods**. The premier example is the [support vector machine](https://en.wikipedia.org/wiki/Support-vector_machine). When taught these algorithms, one is taught that, owing to the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method), one provides the algoirthm with a **kernel** -- that is, a function $$K$$ that operates on pairs of objects that you are classifying and that encodes some measure of "similarity" between your objects. 

That is, if $$\mathcal{X}$$ is the set of objects we are classifying, then a kernel is a positive-definition function:

$$K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$$

That is, for a given pair of items $x_1, x_2 \in \mathcal{X}$, the $K(x_1, x_2)$ is some measure of similarity between $x_1$ and $x_2$.

There are a number of popular kernels that one can use, such as a radial-basis function kernel or polynomial kernel, that are implemented in common machine learning libraries like [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC).  

If you're like me, the kernel was quite mysterious when I first saw it.  This mystery was not solved when beginning to delve into the mathematics behind it, which immediately requires an understanding of another perhaps more mysterious mathematical object called a **reproducing kernel Hilbert space** (RKHS).  

For the purposes of better understanding kernel methods in machine learning, in this post, I will explain the definition of a RKHS and some of its properties in order to provide a foundation for understanding kernel methods like support vector machines.  

The reproducing kernel Hilbert space
------------------------------------





<span style="color:#0060C6">**Theorem 1 (Convergence of functions implies pointwise convergence):** Given a RKHS, $\mathcal{H}$, the following holds: $ \ \lim_{n \rightarrow \infty} \vert\vert f_n - f \vert\vert_{\mathcal{H}} = 0 \implies \forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$.</span>

We assume that we have a convergent sequence of functions that converges on some function $f$. That is, $\lim_{n \rightarrow \infty} \|f_n - f\|_{\mathcal{H}} = 0$. By the definition of the limit of a sequence, this means that

$$\forall \epsilon_2 > 0, \ \exists N \ \text{such that} \ n > N \implies  \vert\vert f_n - f  \vert\vert_{\mathcal{H}} < \epsilon_2$$

Let's keep this in mind as we look at the axioms of a RKHS. Specifically, we note that the axiom of RKHS means that 

$$\forall x \in \mathbb{R}, \ \forall \epsilon_1 > 0, \ \exists \Delta > 0 \ \text{such that} \ \forall f, \ 0 \lt \vert\vert f-g \vert\vert_{\mathcal{H}} \lt \Delta \implies \vert\delta_x(f) - \delta_x(g)\vert < \epsilon_1$$

Let us fix $\epsilon_1$ above to an arbitrary value and let $\epsilon_2 := \Delta$. Furthermore, let $x$ be a fixed arbitrary value. Then, because we have a convergence sequence of functions, $f_n$, we know that $\exists N \ \text{such that} \ n > N \implies \vert\vert f_n - f \vert\vert_{\mathcal{H}} < \Delta$. 

Based on the axiom of the RKHS, this also implies that 

$$\vert \delta_x(f_n) - \delta_x(f)\vert < \epsilon_1$$

and thus,

$$\vert f_n(x) - f(x)\vert < \epsilon_1$$

Now, since our choices of $\epsilon_1$ and $x$ were arbitrary, we see that 

$$\forall x, \ \forall \epsilon_1 > 0, \ \exists N \ \text{such that} \ n > N \implies \vert f_n(x) - f(x) \vert < \epsilon_1$$

This is the very definition of the limit, 

$$\forall x, \ \lim_{n \rightarrow \infty} \vert f_n(x) - f(x) \vert = 0$$

$\square$
