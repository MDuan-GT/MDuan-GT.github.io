---
title: 'Foundations of information theory: what is information? (part 2)'
date: 2020-08-07
permalink: /posts/entropy/
tags:
  - information theory
  - tutorial
---
*The mathematical field of information theory attempts to mathematically describe the concept of “information”. In this series of posts, I will attempt to describe my understanding of how, both philosophically and mathematically, information theory defines the polymorphic, and often amorphous, concept of information. In the first post, we discussed the concept of self-information. In this second post, we will build on this foundation to discuss the concept of information entropy.*

Introduction
-----------

In the [first post](https://mbernste.github.io/posts/self_info/) of this series, we discussed how Shannon's Information Theory defines the information content of an event as the degree of surprise that an agent experiences when the event occurs. We discussed how surprise intuitively should correspond to probability in that an event with low probability elicits more surprise because it is unlikely to occur.  Thus, in information theory, information is a function, $I$, called **self-information**, that operates on probability values $p \in [0,1]$:

$$I(p) := -\log p$$

One strange thing about this equation is that it seems to change with respect to the base of the logarithm. Why is that? In this post, we will connect this idea of "information as surprise" to another angle from which we can view information: "information as an efficiency of communication".  This perspective starts to take more shape when we introduce the concept of information **entropy**.  After introducing this concept, it will become clear that the base of the logarithm used in $I$ corresponds to the number of symbols that we assume we are utilizing to communicate the outcome of the event in question.

Given an event within a [probability space](), self-information describes the information content inherent in that event occuring. The concept of **information entropy** extends this idea to discrete random variables.  Given a random variable $X$, the entropy of $X$, denoted $H(X)$ is simply the expected self-information over its outcomes:

$$H(X) := E\left[I(P(X))\right] = -\sum_{x \in \mathcal{X}}P(X)\log P(X)$$

where $P$ is the probability mass function of $X$ and $\mathcal{X}$ is the codomain of $X$.

Said differently, the entropy of $X$ is simply the average self-information over all of the possible outcomes of $X$.  Intuitively, since self-information describes the degree of surprise of an event, the entropy of a random variable tells us, on average, how surprised we are going to be by the outcome of the random variable.

In the next two subsections we'll discuss two angles from which to view information entropy. The first angle views entropy as a degree of uniformness of a random variable. The second angle views entropy as a limit to how efficiently we can communicate the outcome of this random variable. This latter angle will provide some insight into how to understand the logarithm in the self-information function. 

Entropy measures uniformness
-----------------------

Intuitively, the degree of surprise that we expect to experience from the outcome of a random variable should correspond to how uniform the random variable is.  That is, we should be more surprised by the outcome of a fair sided coin than we should a biased coin. Let's look at two extreme scenarios:
- A discrete random variable that is certain to be only one value (e.g., with probability 1 $X = a$), the outcome of this random variable would not be surprising at all -- we already know its outcome! Therefore, it's entropy should be zero.
- In contrast, a uniform discrete random variable (such as one describing a fair-coin), will always surprise us because we have no idea which of the outcomes will occur -- they are all equally likely!  Intuitively, a uniform random variable should have a high entropy.  In fact, the entropy of a discrete random variable $X$ is maximal when probabilities is uniformly distributed over its outcomes.

In the figure below, we plot the entropy of a Bernoulli random variable $X$ (i.e. a coin flip) over all probabilities of $P(X=1)$:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/entropy_bernoulli.png" alt="drawing" width="500"/></center>

One last way to think about entropy is that entropy tells you how uniformly distributed a discrete random variable is.  A random variable with high entropy is closer to uniform whereas a random variable with low entropy is less uniform (i.e. there is a high probability associated with only a few of its outcomes).  This is depicted in the schematic below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/entropy_levels.png" alt="drawing" width="800"/></center>

Entropy measures maximal efficiency communication
---------------------

I think the most interesting way of viewing entropy is through a lense that involves communication.  More specifically, the information entropy tells you, on average, the minimum number of symbols that you will need to use to communicate the outcome of a random variable. In fact, it was through this lense that Claude Shannon [originally presented the idea](http://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf).  

Let us say we have two people, Person A and Person B, who are trying to communicate with one another. Specifically, Person A is observing samples, drawn one at a time, from some distribution $$X$$.  Person A then wishes to communicate each sample to Person B.  For example, $$X$$ might be a coin and Person A wishes to communicate to Person B the outcomes of repeated coin flips. 

The catch is that Person A must use a sequence of symbols from some alphabet to communicate these outcomes.  For example, Person A may be restricted to communicate using only two symbols, say "1" and "0" (e.g. [Morse Code](https://en.wikipedia.org/wiki/Morse_code) messages are transmitted as using two symobls: dots and dashes). Using these symbols, Person A must construct a code made from these symbols to communicate these outcomes.  

Interestingly, according to [Shannon's Source Coding Theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem), no matter how Person A construct's their code, in expectation, Person A will need use at least $$H(X)$$ symbols to communicate each outcome.  No matter how clever, Person A will never be able to construct a code such that Person A's average message will be smaller than $$H(X)$$.  Said differently, entropy provides a lower bound on the average size of each message that Person A transmits to Person B. 

As it turns out in the Source Coding Theorem leads us to interpret the base of the logarithm used in the definition of $$I$$ as the number of symbols in the alphabet that Person A is using to construct their messages.  Said differently, the base of the logarithm in the definition for $$I$$ can be understood as the size of a hypothetical alphabet that we are using to communicate the result of a surprising event.  

I like to think about it this way: Information quantifies the **surprise** of an event and is measured in units of **alphabet size**.

This is somewhat analagous to money.  Money is used to quantify **value** and is measured in units of a **demonination** (e.g. dollars).

In a future post, I hope to make this connection more clear by rigorously outlining the Source Coding Thoerem. Until then, I hope this series of posts helped provide a broad, and somewhat philosophical, explanation of what "information" is according to Shannon's Theory! 
