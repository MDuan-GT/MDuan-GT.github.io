A high-level connection between intrinsic dimension and entropy
----------------

I really like to make connections between disparate ideas and I think there is, at least at a high level, a connection to be made between the idea of intrinsic dimensionality and information entropy. 

In [another post](), I discuss the idea of information entropy as a description of the information content of a random variable. One way to view entropy, via Shannon's Source Coding Theorem, is as the most one can possibly compress the samples from a distribution when attempting to communicate those samples over a communication channel (see my previous [post] for a rigorous discussion of this idea).
