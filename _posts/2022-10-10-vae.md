---
title: 'Variational autoencoders'
date: 2022-10-10
permalink: /posts/vae/
tags:
  - tutorial
  - machine learning
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders are a class of probabilistic generative models that utilize neural networks in order to fit complicated probability distributions. 

The model
---------

At it's core, the variational autoencoder (VAE) describes family of probability distributions over real-valued vectors in $\mathbb{R}^J$. The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the mean and variance parameters, $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$, of another normal distribution use to sample $\boldsymbol{x}$. Crucially, we construct $\boldsymbol{\mu}, \boldsymbol{\sigma}$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\mu} &:= f_{\theta}(\boldsymbol{z}) \\ \log \boldsymbol{\sigma^2} &:= g_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma}^2) \end{align*}$$

where $f$ and $g$ are neural networks parameterized by a set of parameters $\theta$. This generative process can be visualized below:


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>


Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ are very simple (just normal distributions), the marginal distribution of $\boldsymbol{x}$ is very complex:


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>


This complexity is a result of the complicated, non-linear mapping between $\boldsymbol{z}$ and $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$ implemented via neural networks!

Inference
---------

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ generated by a VAE. We may be interested in two central tasks:
1. Find the maximum likelihood estimates of $\theta$, the parameters to the neural networks
2. For each, $\boldsymbol{x}\_i$, compute the posterior distribution $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$

Ideally, we would solve for the maximum likelihood estimates of $\theta$ using maximum likelihood estimation; however, this is intractible due to the need to marginalize over $\boldsymbol{z}_i$. Even if we had an estimate for $\theta$, solving for the posterior $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$ using Bayes Theorem is also intractible for the same reason: the denominator requires marginalizing over $\boldsymbol{z}_i$.

Variatonal autoencoders find approximate solutions to thes intractible inference problems using [variational inference](https://mbernste.github.io/posts/variational_inference/). 

Example: Training a VAE on Pokémon images
-----------------------------------------

I implemented a [convolutional](https://en.wikipedia.org/wiki/Convolutional_neural_network) VAE in [PyTorch](https://pytorch.org) and trained it on a database of Pokémon images in an effort to reproduce the results presented in [this blog post](https://hackernoon.com/how-to-autoencode-your-pokémon-6b0f5c7b7d97) by Niyas Mohammed. These data consist of 64x64 pixel images. My VAE used a latent representation of length 512, which is about 1/4 the size of the original images.

After enough training, the algorithm was able to reconstruct Pokemon images that were not included in the training data. Here's an example:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/pokemon_reconstruction_VAE.png" alt="drawing" width="600"/></center>

As you can see, it did pretty well! The reconstructed Jigglypuff looks (more or less) like the original Jigglypuff.

Let's see what happens when we attempt to sample images from $p_\theta(\boldsymbol{x})$. In order to do so, we first sample $\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$ and then sample $\boldsymbol{x} \sim N(f_\theta(\boldsymbol{z}), g_\theta(\boldsymbol{z}))$:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/example_pokemon_generative_process.png" alt="drawing" width="900"/></center>

Unfortunately, these sampled images don't really look like Pokémon to me; however, you can begin to see Pokémon-like shapes forming in them (maybe?). Perhaps with tweaking of the neural networks (or more training), we can train a model that generates better images (to be explored in a future blog post).

Lastly, let's explore the latent space learned by the model. First, let's take the images of Charmander and Charmeleon, and encode them into the latent space by sampling from $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$. Let's let $\boldsymbol{z}_1 and $\boldsymbol{z}_2 be the latent vectors for Charmander and Charmeleon respectively: 

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/encode_charmander_charmelion.png" alt="drawing" width="800"/></center>

Then, let's interpolate between $\boldsymbol{z}_1$ and $\boldsymbol{z}_2 and for each interpolated vector $\boldsymbol{z}'$ we'll sample from $\boldsymbol{x} \sim p(\boldsymbol{x} \mid \boldsymbol{z}')$:  

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/charmander_to_charmelion_latent_space.png" alt="drawing" width="800"/></center>

Interestingly, we see a smooth transition between Charmander and Charmeleon (as if he is evolving)!

