---
title: 'Variational Autoencoders'
date: 2022-10-10
permalink: /posts/vae/
tags:
  - tutorial
  - machine learning
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders are a class of probabilistic generative models that utilize neural networks in order to fit complicated probability distributions. 

The model
---------

At it's core, the variational autoencoder (VAE) is a family of probability distributions over real-valued vectors in $\mathbb{R}^J$. The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the mean and variance parameters, $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$, of another normal distribution use to sample $\boldsymbol{x}$. Crucially, we construct $\boldsymbol{\mu}, \boldsymbol{\sigma}$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\mu} &:= f(\boldsymbol{z}) \\ \log \boldsymbol{\sigma^2} &:= g(\boldsymbol{z}) \\ \boldsymbol{x} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma}^2) \end{align*}$$

where $f$ and $g$ are neural networks. Before moving on, let's examine the basic structure of this model.
