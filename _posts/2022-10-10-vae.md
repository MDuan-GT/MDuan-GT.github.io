---
title: 'Variational Autoencoders'
date: 2022-10-10
permalink: /posts/vae/
tags:
  - tutorial
  - machine learning
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders are a class of probabilistic generative models that utilize neural networks in order to fit complicated probability distributions. 

The model
---------

At it's core, the variational autoencoder (VAE) is a family of probability distributions over real-valued vectors in $\mathbb{R}^J$. The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first sample a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$). Typically, $\boldsymbol{z}$ is made to follow a  Then, given $\boldsymbol{z}_i$ we generate $\boldsymbol{x}_i$. Thus, the VAE assumes the following generative process:

$$\begin{align*}\boldsymbol{z}_i \sim Z \ \boldsymbol{x}_i \sim X \mid \boldsymbol{z}_i \end{align*}$$

where $Z$ is the distribution over each $\boldsymbol{z}_i$ and $X \mid \boldsymbol{z}_i$ is the conditional distribution of $\boldsymbol{x}_i$ given $\boldsymbol{z}_i$. Thus, the joint distribution of e
