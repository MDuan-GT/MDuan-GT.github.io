---
title: 'Variational Autoencoders'
date: 2022-10-10
permalink: /posts/vae/
tags:
  - tutorial
  - machine learning
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders are a class of probabilistic generative models that utilize neural networks in order to fit complicated probability distributions. 

The model
---------

At it's core, the variational autoencoder (VAE) describes family of probability distributions over real-valued vectors in $\mathbb{R}^J$. The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the mean and variance parameters, $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$, of another normal distribution use to sample $\boldsymbol{x}$. Crucially, we construct $\boldsymbol{\mu}, \boldsymbol{\sigma}$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\mu} &:= f_{\theta}(\boldsymbol{z}) \\ \log \boldsymbol{\sigma^2} &:= g_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma}^2) \end{align*}$$

where $f$ and $g$ are neural networks parameterized by a set of parameters $\theta$. This generative process can be visualized below:


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>


Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ are very simple (just normal distributions), the marginal distribution of $\boldsymbol{x}$ is very complex:


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>


This complexity is a result of the complicated, non-linear mapping between $\boldsymbol{z}$ and $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$ implemented via neural networks!

Inference
---------

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ generated by a VAE. We may be interested in two central tasks:
1. For each, $\boldsymbol{x}_i$, compute the posterior distribution $p(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$
2. Find the maximum likelihood estimates of $\theta$, the parameters to the neural networks



