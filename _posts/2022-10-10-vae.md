---
title: 'Variational autoencoders'
date: 2022-10-10
permalink: /posts/vae/
tags:
  - tutorial
  - machine learning
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders are a class of probabilistic models that utilize neural networks to model latent, low-dimensional representations of data. There are two central ways of viewing variational autoencoders:
1. **Probabilistic generative model:** Variational autoencoders can be viewed as a probabilistic generative model of independent, identically distributed samples, $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m$ where each sample, $\boldsymbol{x}_i$, is associated with a lower-dimensional representation $\boldsymbol{z}_i$. 
2. **Autoencoder:** 

VAEs as probabilistic generative models
---------------------------------------

At it's core, the variational autoencoder (VAE) describes family of probability distributions over real-valued vectors in $\mathbb{R}^J$. The generative process behind the VAE is as follows: to generate a given sample $\boldsymbol{x} \in \mathbb{R}^J$, we first generate a latent sample  $\boldsymbol{z} \in \mathbb{R}^{D}$ of lower dimension (i.e.,  $D < J$) which will be used to construct $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the parameters, $\boldsymbol{\psi}$, of another distribution used to sample $\boldsymbol{x}$. Crucially, we construct $\psi$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\psi} &:= f_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim D(\boldsymbol{\psi}) \end{align*}$$

where $D$ is a parametric distribution and $f$ is a neural network parameterized by a set of parameters $\theta$. As one example, $D$ may be a Gaussian distribution with unit variance and $\psi$ is the mean of this distribution. This generative process can be visualized below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>


Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ are very simple (just normal distributions), the marginal distribution of $\boldsymbol{x}$ is very complex:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>

This complexity is a result of the complicated, non-linear mapping between $\boldsymbol{z}$ and $\boldsymbol{\mu}, \boldsymbol{\sigma}^2$ implemented via neural networks!

Inference
---------

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ generated by a VAE. We may be interested in two central tasks:
1. Find the maximum likelihood estimates of $\theta$, the parameters to the neural networks
2. For each, $\boldsymbol{x}\_i$, compute the posterior distribution $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$

Ideally, we would estimate the values for $\theta$ by maximizing the data likelihood:

$$\hat{\theta} := \text{argmax}_\theta \prod_{i=1}^m p_\theta(\boldsymbol{x}_i)$$

Unfortunately, this is intractible due to the need to marginalize over each $\boldsymbol{z}\_i$. Even if we had an estimate for $\theta$, solving for the posterior $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$ using Bayes Theorem is also intractible for the same reason: the denominator in the formula for Bayes Theorem requires marginalizing over $\boldsymbol{z}_i$.

Variatonal autoencoders find approximate solutions to these intractible inference problems using [variational inference](https://mbernste.github.io/posts/variational_inference/). Recall, variational inference entails choosing a set of probability distributions $\mathcal{Q}$, called the variational family, and then finding the distribution $q(\boldsymbol{z}) \in \mathcal{Q}$ that is "closest to" the true posterior $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$. Variational inference uses the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $q(\boldsymbol{z})$ and $p_\theta(\boldsymbol{z} \mid \boldsymbol{x})$ as a measure of "closeness". Thus, the goal of variational inference is to minimize the KL-divergence. It turns out that the task of minimizing the KL-divergence is equivalent to the task of maximizing a quantity called the [evidence lower bound (ELBO)](https://mbernste.github.io/posts/elbo/), which is defined as

$$\text{ELBO}(q) := E_{\boldsymbol{z} \sim q}\left[ \sum_{i=1}^m \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \sum_{i=1}^m \log q(\boldsymbol{z}_i) \right]$$

Thus, variational inference entails finding

$$\hat{q} := \text{arg max}_{q \in \mathcal{Q}} \ \text{ELBO}(q)$$

Now, so far we have assumed that $\theta$ is fixed. Is it possible to find both $q$ and $\theta$ jointly? It turns out the answer is yes! We can simply define the ELBO to be a function of _both_ $q$ and $\theta$ and then maximize the ELBO jointly:

$$\hat{q}, \hat{\theta} := \text{arg max}_{q, \theta} \ \text{ELBO}(q, \theta)$$

Why does this work? Recall the evidence lower bound is a lower bound for $\log p_\theta(\boldsymbol{x})$

### Variational family

Variational autoencoders use a variational family with the following form:

$$\mathcal{Q} := \{N(g_\phi(\boldsymbol{x}), \exp(h_\phi(\boldsymbol{x})) \boldsymbol{I})\}$$

where $g_{\phi}$ and $h_{\phi}$ are neural networks that encode the mean, $\boldsymbol{\mu}$ the logarithm of the variance, $\log \boldsymbol{\sigma}^2$ of the approximate posterior. Said a second way, we define $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ as

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(g_\phi(\boldsymbol{x}), \exp(h_\phi(\boldsymbol{x})) \boldsymbol{I})$$

Said a third way, the posterior distribution can be sampled via the following generative process:

$$\begin{align*}\boldsymbol{\mu} :=  g_\phi(\boldsymbol{x})\end{align*}$$
$$\begin{align*}\boldsymbol{\mu} :=  g_\phi(\boldsymbol{x})\end{align*}$$

Thus, maximizing the ELBO over $\mathcal{Q}$ reduces to maximizing over the neural network parameters $\phi$ (in addition to $\theta$ as discussed previously):

$$\begin{align*}\hat{\phi}, \hat{\theta} &:= \text{arg max}_{\phi, \theta} \ E_{\boldsymbol{z} \sim q_\phi}\left[ \sum_{i=1}^m \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \sum_{i=1}^m \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \text{arg max}_{\phi} \  \text{ELBO}(\phi)\end{align*}$$

### Optimizing the ELBO


VAEs as autoencoders
---------------------

Example: Training a VAE on MNIST
--------------------------------

I implemented a [convolutional](https://en.wikipedia.org/wiki/Convolutional_neural_network) VAE in [PyTorch](https://pytorch.org) and trained it on a database of Pokémon images in an effort to reproduce the results presented in [this blog post](https://hackernoon.com/how-to-autoencode-your-pokémon-6b0f5c7b7d97) by Niyas Mohammed. These data consist of 64x64x3 pixel images. My VAE used a latent representation of length 512 (that is, $\boldsymbol{z} \in \mathbb{R}^{512}$.

After enough training, the algorithm was able to reconstruct Pokemon images that were not included in the training data. Here's an example:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/pokemon_reconstruction_VAE.png" alt="drawing" width="600"/></center>

As you can see, it did pretty well! The reconstructed Jigglypuff looks (more or less) like the original Jigglypuff.

Let's see what happens when we attempt to sample images from $p_\theta(\boldsymbol{x})$. In order to do so, we first sample $\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$ and then sample $\boldsymbol{x} \sim N(f_\theta(\boldsymbol{z}), g_\theta(\boldsymbol{z}))$:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/example_pokemon_generative_process.png" alt="drawing" width="900"/></center>

Unfortunately, these sampled images don't really look like Pokémon to me; however, you can begin to see Pokémon-like shapes forming in them (maybe?). Perhaps with tweaking of the neural networks (or more training), we can train a model that generates better images (to be explored in a future blog post).

Lastly, let's explore the latent space learned by the model. First, let's take the images of Charmander and Charmeleon, and encode them into the latent space by sampling from $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$. Let's let $\boldsymbol{z}_1$ and $\boldsymbol{z}_2 be the latent vectors for Charmander and Charmeleon respectively: 

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/encode_charmander_charmelion.png" alt="drawing" width="800"/></center>

Then, let's interpolate between $\boldsymbol{z}_1$ and $\boldsymbol{z}_2$ and for each interpolated vector $\boldsymbol{z}'$ we'll sample from $\boldsymbol{x} \sim p(\boldsymbol{x} \mid \boldsymbol{z}')$:  

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/charmander_to_charmelion_latent_space.png" alt="drawing" width="800"/></center>

Interestingly, we see a smooth transition between Charmander and Charmeleon (as if he is evolving)!

